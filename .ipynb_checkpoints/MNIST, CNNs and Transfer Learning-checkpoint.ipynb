{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-33450cbf2a0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m \u001b[0;31m# data processing, CSV file I/O (e.g. pd.read_csv)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Conv2D , Dropout, Activation, Flatten, Input, MaxPooling2D\n",
    "from keras.utils import to_categorical, plot_model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "#for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    #for filename in filenames:\n",
    "        #print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Goal\n",
    "\n",
    "The goal of this notebook is to see how MLPs compare to CNNs in terms of digit recognition. \n",
    "\n",
    "Additionally we will see the  generalization capabilities of CNNs. For this last step, we will train the CNN in data for digits and see how they generalize after taking the same networks to be tested on the alphabet characters and clothing items from the \"Fashion MNIST\" dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and analyse standard MNIST data\n",
    "\n",
    "Let us start by importing the standard MNIST dataset, comprised of 60 000 train samples, with balanced data, for digits from 1 to 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "numbers_df=pd.read_csv('emnist/emnist-mnist-train.csv',header=None)\n",
    "test_df=pd.read_csv('emnist/emnist-mnist-test.csv',header=None)\n",
    "numbers_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, 60 000 samples, with 784 (28x28) features. The first column in the dataset is the label for the training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many of each digit class is there in the training ans test sets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(data=numbers_df,x=0,palette='pastel')\n",
    "sns.countplot(data=test_df,x=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 6000 of each class in the training set and 1000 of each in the test set.\n",
    "\n",
    "!! Carefull !! The picture above is not a stacked countplot!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shoul try to visualize some of these digits. Let us make a function for that purpose, since we may want to reuse it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis(dataset,nr_samples,label_col,legend=False,cmap='gray_r',cbar=False,transpose=True,prediction=False,dict_name=None):\n",
    "    samples=dataset.iloc[np.random.randint(0,dataset.shape[0],size=(1,nr_samples))[0]].reset_index(drop=True) # Picks n random samples from the dataset\n",
    "    labels=samples[label_col].values\n",
    "    samples.drop(label_col,axis=1,inplace=True)\n",
    "    if prediction==True:\n",
    "        preds=samples['Prediction'].values\n",
    "        samples.drop('Prediction',axis=1,inplace=True)\n",
    "    fig, ax = plt.subplots(2,nr_samples//2,sharey=True,sharex=True)\n",
    "    for i in range(nr_samples):\n",
    "        pixels=samples.iloc[i].values\n",
    "        pixels=pixels.reshape((28,28))\n",
    "        if transpose==True:\n",
    "            pixels=pixels.transpose()\n",
    "        sns.heatmap(pixels,cmap=cmap,cbar=cbar,ax=ax.flatten()[i])\n",
    "        ax.flatten()[i].axes.get_xaxis().set_visible(False)\n",
    "        ax.flatten()[i].axes.get_yaxis().set_visible(False)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    if legend==True:\n",
    "        if dict_name!=None:\n",
    "            labels=[dict_name[i] for i in labels]\n",
    "        if prediction==True:\n",
    "            print('The images represent items with labels {} which were predicted to be {}.'.format(labels,preds))\n",
    "        else:\n",
    "            print('The images represent items with labels {}.'.format(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now see the representation of some samples from the numbers_df dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis(numbers_df,10,label_col=0,legend=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** <font size=\"5\">Data Processing</font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having visualised the dataset, and checked some digits, we should start processing our data. The first step is to normalize the values of the geyscale. As can be seen in the scale of the previous Figure, values are ranging from 0 (for black) to 255 (for white). We'll normalize them, so they are kept in the interval $\\left[0,1\\right]$.\n",
    "\n",
    "For this instance in particular, we will use a MinMaxScaler fit on the train set and then fit it to the testing set as well. This could be done instead dividing by 255, but this procedure is more general. Further down in the notebook, we will just divide by 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df=numbers_df.drop(0,axis=1)\n",
    "labels_df=numbers_df[0]\n",
    "scaler=MinMaxScaler(feature_range=(0,1))\n",
    "scaler.fit(features_df)\n",
    "features_df=pd.DataFrame(scaler.transform(features_df),columns=features_df.columns)\n",
    "features_df['Label']=labels_df\n",
    "features_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try to visualize the numbers with the rescaled features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis(features_df,10,label_col='Label',legend=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scaling was done correctly, as the numbers didn't get distorted.\n",
    "We now use the scaler which was fitted to the training set to scale the test set as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features_df=test_df.drop(0,axis=1)\n",
    "test_labels_df=test_df[0]\n",
    "test_features_df=pd.DataFrame(scaler.transform(test_features_df),columns=test_features_df.columns)\n",
    "test_features_df['Label']=test_labels_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having done this scaling, we are ready to use this to predict the handwriten digits!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Layer Perceptron (MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we define the building of the MLP model using a function. This helps in testing different architectures in a a faster and more efficient way. It will also allow for the specification of dropout layers and control the number of classes in an easy way, by specifying function arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLP(input_shape,nodes=[128,32],dropout_chance=0.4,num_classes=10,produce_output=True):\n",
    "    # If a 0 is inserted into the nodes, that means a Dropout layer is to be added\n",
    "    model=Sequential()\n",
    "    model.add(Dense(nodes[0],input_shape=input_shape,activation='relu'))\n",
    "    num_hidd_layers=len(nodes)\n",
    "    for i in range(num_hidd_layers-1):\n",
    "        if nodes[i+1]==0:\n",
    "            model.add(Dropout(dropout_chance))\n",
    "        else:\n",
    "            model.add(Dense(nodes[i+1],activation='relu'))\n",
    "    if produce_output==True:\n",
    "        model.add(Dense(num_classes,activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define the model. It will be a four-layer model. The first hidden layer has 256 nodes, with 'ReLu' as activation function, the second has 128 nodes, also with 'ReLu' and the third has 32 nodes. The output layer has 10 nodes and it uses softmax as activation, since it allows for a probabilistic interpretation of the outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model=MLP((784,),nodes=[256,128,32])\n",
    "\n",
    "# Compile and fit the model\n",
    "model.compile(loss='categorical_crossentropy',optimizer='rmsprop',metrics=['accuracy'])\n",
    "history = model.fit(features_df.drop('Label',axis=1),to_categorical(features_df['Label']),nb_epoch=25,validation_split=0.2,batch_size=128,verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see the learning curve for this model. \n",
    "(As this will be used often, we'll define a function for it.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_curve(history,titles,legend,metrics=['accuracy','loss']):\n",
    "    nplots=len(metrics)\n",
    "    fig , ax = plt.subplots(1,nplots,sharex=True)\n",
    "    fig.set_figheight(5)\n",
    "    fig.set_figwidth(15)\n",
    "    for i in range(nplots):\n",
    "        ax[i].plot(history.history[metrics[i]])\n",
    "        ax[i].plot(history.history['val_{}'.format(metrics[i])])\n",
    "        ax[i].set(xlabel='Epoch', ylabel=metrics[i])\n",
    "        ax[i].legend(legend)\n",
    "        ax[i].title.set_text(titles[i])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_curve(history,['Model Accuracy','Model Loss'],legend=['Train','Test'],metrics=['accuracy','loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is fitted to the training data, we'll test it in the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = model.predict_classes(test_features_df.drop('Label',axis=1))\n",
    "score, acc = model.evaluate(test_features_df.drop('Label',axis=1),to_categorical(test_features_df['Label']))\n",
    "print('The loss of the test data is {:.3f} with an accuracy of {:.3f}.'.format(score, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many misslabeled samples do we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds_df=pd.DataFrame()\n",
    "test_preds_df['Prediction']=test_preds\n",
    "test_preds_df['Label']=test_features_df['Label']\n",
    "missclassified=test_preds_df[test_preds_df.Prediction!=test_preds_df.Label]\n",
    "missclassified_index=missclassified.index.to_list()\n",
    "print('{} images (out of 10 000) were missclassified!'.format(len(missclassified_index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have some wrongly classified samples. Let's see some of them in order to try and understand why they occurred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missclassified_features_df=test_features_df.iloc[missclassified_index].copy()\n",
    "missclassified_features_df['Prediction']=missclassified['Prediction']\n",
    "missclassified_features_df.reset_index(inplace=True,drop=True)\n",
    "missclassified_features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis(missclassified_features_df,10,'Label',legend=True,prediction=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can se, some of the missclassified digits are not difficult to recognize by human standards. Let's see if a CNN can achieve better results through it's pattern recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, just like in the MLP, we create the model. Again, let's make it a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN(input_shape,num_kernels=[20,20],kernel_shapes=[(3,3),(3,3)],dense_nodes=[128],dropout_chance=0.4,num_classes=10,produce_output=True):\n",
    "    # A 0 inserted either in num_kernels or in dense_nodes means a Dropout layer is to be inserted at that point\n",
    "    # If it is inserted in the convolutional layers, then some value must be adde in the corresponding place in kernel_shapes\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(num_kernels[0],kernel_size=kernel_shapes[0],activation='relu',input_shape=input_shape))\n",
    "    num_conv_layers = len(num_kernels)\n",
    "    for i in range(num_conv_layers-1):\n",
    "        if num_kernels[i+1]==0:\n",
    "            model.add(Dropout(dropout_chance))\n",
    "        else:\n",
    "            model.add(Conv2D(num_kernels[i+1],kernel_size=kernel_shapes[i+1],activation='relu'))\n",
    "            model.add(MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None))\n",
    "    num_dense_layers = len(dense_nodes)\n",
    "    model.add(Flatten())\n",
    "    for j in range(num_dense_layers):\n",
    "        if dense_nodes[j]==0:\n",
    "            model.add(Dropout(dropout_chance))\n",
    "        else:\n",
    "            model.add(Dense(dense_nodes[j],activation='relu'))\n",
    "    if produce_output==True:\n",
    "        model.add(Dense(num_classes,activation='softmax'))\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use a model which has the same structure in the dense part as the former MLP, but as a first step it encodes the features found by using convolutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we prepare the data for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images=features_df.shape[0]\n",
    "X=features_df.drop('Label',axis=1).values.reshape(num_images,28,28,1)\n",
    "y=features_df['Label'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we compile and fit the model with the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "cnn_model = CNN((28,28,1),num_kernels=[20,30],kernel_shapes=[(3,3),(4,4)],dense_nodes=[256,128,32])\n",
    "\n",
    "# Compile and fit the model\n",
    "cnn_model.compile(loss = 'categorical_crossentropy',optimizer = 'rmsprop',metrics = ['accuracy'])\n",
    "cnn_history = cnn_model.fit(X,to_categorical(y),nb_epoch = 25,validation_split = 0.2,batch_size = 128,verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before proceeding, one should check for the possibility of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_curve(cnn_history,['Model Accuracy','Model Loss'],legend=['Train','Test'],metrics=['accuracy','loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many digits were missclassified with the CNN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images=test_features_df.shape[0]\n",
    "X_test=test_features_df.drop('Label',axis=1).values.reshape(test_images,28,28,1)\n",
    "y_test=test_features_df['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_test_preds = cnn_model.predict_classes(X_test)\n",
    "score, acc = cnn_model.evaluate(X_test,to_categorical(y_test))\n",
    "print('The loss of the test data is {:.3f} with an accuracy of {:.3f}.'.format(score, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_test_preds_df=pd.DataFrame()\n",
    "cnn_test_preds_df['Prediction']=cnn_test_preds\n",
    "cnn_test_preds_df['Label']=test_features_df['Label']\n",
    "cnn_missclassified=cnn_test_preds_df[cnn_test_preds_df.Prediction!=cnn_test_preds_df.Label]\n",
    "cnn_missclassified_index=cnn_missclassified.index.to_list()\n",
    "print('{} images (out of 10 000) were missclassified!'.format(len(cnn_missclassified_index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have some improvements! The CNN is able to classify correctly more samples than the simple MLP. Let's check which were missed by the CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_missclassified_features_df=test_features_df.iloc[cnn_missclassified_index].copy()\n",
    "cnn_missclassified_features_df['Prediction']=cnn_missclassified['Prediction']\n",
    "cnn_missclassified_features_df.reset_index(inplace=True,drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis(cnn_missclassified_features_df,10,'Label',legend=True,prediction=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at some sample images, it is possible to notice that the images in which the CNN fails to classify the digit correctly are much more prone to be wrongly classified by humans too: the prediction and the label don't mach but many times the drawn digit resembles the prediction in some way. This does not happen so much with the MLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, we can notice some overfitting by looking at the learning curve of both models. This can be countered with the addition of Dropout layers. These were purposefully not added so one can get a sense of their effect.\n",
    "\n",
    "Let us now add these Dropout layers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP with Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following model, we introduce some Dropout layers, to see if we can minimize the overfitting effect we have in the standard MLP model above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "model_do=MLP((784,),nodes=[256,0,128,32])\n",
    "\n",
    "# Compile and fit model\n",
    "model_do.compile(loss='categorical_crossentropy',optimizer='rmsprop',metrics=['accuracy'])\n",
    "history_do = model_do.fit(features_df.drop('Label',axis=1),to_categorical(features_df['Label']),nb_epoch=25,validation_split=0.2,batch_size=128,verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the learning curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_curve(history_do,['Model Accuracy','Model Loss'],legend=['Train','Test'],metrics=['accuracy','loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it seems that both curves are converging, unlike what happened without Dropout layers. Furthermore, this comes at virtually no cost for the accuracy of the model and a slight decrease in loss. \n",
    "\n",
    "Let's see how is the performace on the test set with this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = model_do.predict_classes(test_features_df.drop('Label',axis=1))\n",
    "score, acc = model_do.evaluate(test_features_df.drop('Label',axis=1),to_categorical(test_features_df['Label']))\n",
    "print('The loss of the test data is {:.3f} with an accuracy of {:.3f}.'.format(score, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds_df=pd.DataFrame()\n",
    "test_preds_df['Prediction']=test_preds\n",
    "test_preds_df['Label']=test_features_df['Label']\n",
    "missclassified=test_preds_df[test_preds_df.Prediction!=test_preds_df.Label]\n",
    "missclassified_index=missclassified.index.to_list()\n",
    "print('{} images (out of 10 000) were missclassified!'.format(len(missclassified_index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen in the validation scores, we get a similar result, but now with a far more consistent learning through the several epochs of training, with the validation accuracy and loss converging to that of the training ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN with Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "cnn_model_do = CNN((28,28,1),num_kernels=[20,30],kernel_shapes=[(3,3),(4,4)],dense_nodes=[0,256,0,128,32])\n",
    "\n",
    "# Compile and fit model\n",
    "cnn_model_do.compile(loss = 'categorical_crossentropy',optimizer = 'rmsprop',metrics = ['accuracy'])\n",
    "cnn_history_do = cnn_model_do.fit(X,to_categorical(y),nb_epoch = 25,validation_split = 0.2,batch_size = 128,verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should look at the learning curves now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_curve(cnn_history_do,['Model Accuracy','Model Loss'],legend=['Train','Test'],metrics=['accuracy','loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about the model performance on the test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_test_preds = cnn_model_do.predict_classes(X_test)\n",
    "score, acc = cnn_model_do.evaluate(X_test,to_categorical(y_test))\n",
    "print('The loss of the test data is {:.3f} with an accuracy of {:.3f}.'.format(score, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_test_preds_df=pd.DataFrame()\n",
    "cnn_test_preds_df['Prediction']=cnn_test_preds\n",
    "cnn_test_preds_df['Label']=test_features_df['Label']\n",
    "cnn_missclassified=cnn_test_preds_df[cnn_test_preds_df.Prediction!=cnn_test_preds_df.Label]\n",
    "cnn_missclassified_index=cnn_missclassified.index.to_list()\n",
    "print('{} images (out of 10 000) were missclassified!'.format(len(cnn_missclassified_index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the case of the MLP, the introduction of Dropout layers was able to solve the overfitting problem, as can be seen in the learning curves for the above model. In doing this, it improved the performance of our model on the test set as well, since it is now able to generalize consistently to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now check how adaptable CNN are. For that, we will train these networks on the digits dataset and then take the learned weights and build a model for classifying alphabet characters with them.\n",
    "For this effect, using the Sequential() model from Keras is not the ideal. We will therefore proceed to use Functional API model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by importing the alphabet dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_df = pd.read_csv('/kaggle/input/emnist/emnist-letters-train.csv',header=None)\n",
    "test_char_df = pd.read_csv('/kaggle/input/emnist/emnist-letters-test.csv',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization and changing the label column name to 'Label'\n",
    "labels=char_df[0].tolist()\n",
    "char_df.drop(0,axis=1,inplace=True)\n",
    "char_df=char_df/255\n",
    "char_df['Label']=labels\n",
    "\n",
    "test_labels=test_char_df[0].tolist()\n",
    "test_char_df.drop(0,axis=1,inplace=True)\n",
    "test_char_df=test_char_df/255\n",
    "test_char_df['Label']=test_labels\n",
    "test_char_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2)\n",
    "sns.countplot(data=char_df,x='Label',ax=ax[0])\n",
    "ax[0].title.set_text('Training set')\n",
    "sns.countplot(data=test_char_df,x='Label',ax=ax[1])\n",
    "ax[1].title.set_text('Test set')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the set of labels for the training set and the test set is not the same (there are some labels missing from the test set), we will have to increase the length of the one-hot encoded vectors for the test set before applying our model to the them. This will be done at some later point in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis(char_df,10,'Label',legend=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vision part of the model\n",
    "inputs=Input((28,28,1))\n",
    "conv1=Conv2D(20,(3,3),activation='relu')(inputs)\n",
    "conv2=Conv2D(30,(4,4),activation='relu')(conv1)\n",
    "output1=Flatten()(conv2)\n",
    "\n",
    "vision_model=Model(inputs,output1)\n",
    "\n",
    "inputs2=vision_model(inputs)\n",
    "\n",
    "# Specific part of the model for digits\n",
    "\n",
    "dropout1=Dropout(0.4)(inputs2)\n",
    "dense1=Dense(256,activation='relu')(dropout1)\n",
    "dropout2=Dropout(0.4)(dense1)\n",
    "dense2=Dense(128,activation='relu')(dropout2)\n",
    "dense3=Dense(32,activation='relu')(dense2)\n",
    "output2=Dense(10,activation='softmax')(dense3)\n",
    "\n",
    "\n",
    "# Specific part of the model for characters\n",
    "\n",
    "dropout1_c=Dropout(0.4)(inputs2)\n",
    "dense1_c=Dense(256,activation='relu')(dropout1_c)\n",
    "dropout2_c=Dropout(0.4)(dense1_c)\n",
    "dense2_c=Dense(128,activation='relu')(dropout2_c)\n",
    "dense3_c=Dense(32,activation='relu')(dense2_c)\n",
    "output2_c=Dense(27,activation='softmax')(dense3_c)\n",
    "\n",
    "# Specific part of the model for fashion MNIST\n",
    "\n",
    "dropout1_f=Dropout(0.4)(inputs2)\n",
    "dense1_f=Dense(256,activation='relu')(dropout1_f)\n",
    "dropout2_f=Dropout(0.4)(dense1_f)\n",
    "dense2_f=Dense(128,activation='relu')(dropout2_f)\n",
    "dense3_f=Dense(32,activation='relu')(dense2_f)\n",
    "output2_f=Dense(10,activation='softmax')(dense3_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on MNIST Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model layers are now created. Let us define the model for traning on the digits dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model (same as previous CNN with Dropout)\n",
    "training_model=Model(inputs,outputs=output2)\n",
    "\n",
    "# Compile and fit the model\n",
    "training_model.compile(optimizer='rmsprop', loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "training_history = training_model.fit(X,to_categorical(y),nb_epoch = 25,validation_split = 0.2,batch_size = 128,verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_curve(training_history,['Model Accuracy','Model Loss'],legend=['Train','Test'],metrics=['accuracy','loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits_test_probs = training_model.predict(X_test)\n",
    "digits_test_preds = [np.argmax(np.asarray(i)) for i in digits_test_probs]\n",
    "score, acc = training_model.evaluate(X_test,to_categorical(y_test))\n",
    "print('The loss of the test data is {:.3f} with an accuracy of {:.3f}.'.format(score, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits_test_preds_df=pd.DataFrame()\n",
    "digits_test_preds_df['Prediction']=digits_test_preds\n",
    "digits_test_preds_df['Label']=test_features_df['Label']\n",
    "digits_missclassified=digits_test_preds_df[digits_test_preds_df.Prediction!=digits_test_preds_df.Label]\n",
    "digits_missclassified_index=digits_missclassified.index.to_list()\n",
    "print('{} images (out of 10 000) were missclassified!'.format(len(digits_missclassified_index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Letters MNIST data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing character image data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_chars=char_df.shape[0]\n",
    "X_char=char_df.drop('Label',axis=1).values.reshape(num_chars,28,28,1)\n",
    "y_char=char_df['Label'].values\n",
    "\n",
    "num_test_chars=test_char_df.shape[0]\n",
    "test_X_char=test_char_df.drop('Label',axis=1).values.reshape(num_test_chars,28,28,1)\n",
    "test_y_char=test_char_df['Label'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the model for classifying the characters, without training the convolutional part of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1.trainable = False\n",
    "conv2.trainable = False\n",
    "output1.trainable = False\n",
    "\n",
    "char_model=Model(inputs,output2_c)\n",
    "plot_model(char_model,show_shapes=True,show_layer_names=True,expand_nested=True)\n",
    "char_model.compile(optimizer='rmsprop', loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "char_history = char_model.fit(X_char,to_categorical(y_char),nb_epoch = 25,validation_split = 0.2,batch_size = 256,verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_curve(char_history,['Model Accuracy','Model Loss'],legend=['Train','Test'],metrics=['accuracy','loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check performance and missed samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_test_probs = char_model.predict(test_X_char)\n",
    "char_test_preds = [np.argmax(np.asarray(i)) for i in char_test_probs]\n",
    "filler=[0.0,0.0,0.0,0.0,0.0,0.0,0.0] #adds 0s to the labels from 21 to 27 because there are no samples with tese labels in the test set\n",
    "y_vals=np.asarray([list(i)+filler for i in to_categorical(test_y_char)])\n",
    "score_char, acc_char = char_model.evaluate(test_X_char,y_vals)\n",
    "print('The loss of the test data is {:.3f} with an accuracy of {:.3f}.'.format(score_char, acc_char))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_test_preds_df=pd.DataFrame()\n",
    "char_test_preds_df['Prediction']=char_test_preds\n",
    "char_test_preds_df['Label']=test_char_df['Label']\n",
    "char_missclassified=char_test_preds_df[char_test_preds_df.Prediction!=char_test_preds_df.Label]\n",
    "char_missclassified_index=char_missclassified.index.to_list()\n",
    "print('{} images (out of {}) were missclassified!'.format(len(char_missclassified_index),test_char_df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fashion MNIST Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we try to use the same convolution layers as trained for the digit dataset to try and make predictions about the Fashion MNIST data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "fashion_df = pd.read_csv('/kaggle/input/fashionmnist/fashion-mnist_train.csv')\n",
    "test_fashion_df = pd.read_csv('/kaggle/input/fashionmnist/fashion-mnist_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change format\n",
    "labels=fashion_df['label'].tolist()\n",
    "fashion_df.drop('label',axis=1,inplace=True)\n",
    "fashion_df=fashion_df/255\n",
    "fashion_df['Label']=labels\n",
    "\n",
    "test_labels_f=test_fashion_df['label'].tolist()\n",
    "test_fashion_df.drop('label',axis=1,inplace=True)\n",
    "test_fashion_df=test_fashion_df/255\n",
    "test_fashion_df['Label']=test_labels_f\n",
    "fashion_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2)\n",
    "sns.countplot(data=fashion_df,x='Label',ax=ax[0])\n",
    "ax[0].title.set_text('Training set')\n",
    "sns.countplot(data=test_fashion_df,x='Label',ax=ax[1])\n",
    "ax[1].title.set_text('Test set')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, both sets have the same labesl, so there is no need to complete the one-hot encoded vectors for any of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_items=fashion_df.shape[0]\n",
    "X_fashion=fashion_df.drop('Label',axis=1).values.reshape(num_items,28,28,1)\n",
    "y_fashion=fashion_df['Label'].values\n",
    "\n",
    "num_test_items=test_fashion_df.shape[0]\n",
    "test_X_fashion=test_fashion_df.drop('Label',axis=1).values.reshape(num_test_items,28,28,1)\n",
    "test_y_fashion=test_fashion_df['Label'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize some of these images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary for converting items into names.\n",
    "\n",
    "fashion_dict={0: 'T-shirt/top', 1:'Trouser', 2: 'Pullover', 3: 'Dress', 4: 'Coat', 5: 'Sandal', 6: 'Shirt', 7: 'Sneaker', 8: 'Bag', 9: 'Ankle boot'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis(fashion_df,10,'Label',legend=True,transpose=False,dict_name=fashion_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define and fit the specific part of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1.trainable = False\n",
    "conv2.trainable = False\n",
    "output1.trainable = False\n",
    "\n",
    "fashion_model=Model(inputs,output2_f)\n",
    "plot_model(fashion_model,show_shapes=True,show_layer_names=True,expand_nested=True)\n",
    "fashion_model.compile(optimizer='rmsprop', loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "fashion_history = fashion_model.fit(X_fashion,to_categorical(y_fashion),nb_epoch = 25,validation_split = 0.2,batch_size = 128,verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_curve(fashion_history,['Model Accuracy','Model Loss'],legend=['Train','Test'],metrics=['accuracy','loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_test_probs = fashion_model.predict(test_X_fashion)\n",
    "fashion_test_preds = [np.argmax(np.asarray(i)) for i in fashion_test_probs]\n",
    "score_fashion, acc_fashion = fashion_model.evaluate(test_X_fashion,to_categorical(test_y_fashion))\n",
    "print('The loss of the test data is {:.3f} with an accuracy of {:.3f}.'.format(score_fashion, acc_fashion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_test_preds_df=pd.DataFrame()\n",
    "fashion_test_preds_df['Prediction']=fashion_test_preds\n",
    "fashion_test_preds_df['Label']=test_fashion_df['Label']\n",
    "fashion_missclassified=fashion_test_preds_df[fashion_test_preds_df.Prediction!=fashion_test_preds_df.Label]\n",
    "fashion_missclassified_index=fashion_missclassified.index.to_list()\n",
    "print('{} images (out of {}) were missclassified!'.format(len(fashion_missclassified_index),test_fashion_df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By analysing the learning curves of the letters and fashion models, one is able to notice the existence of overfitting. Additionally, it is possible to see that there is some room for improvement, even in the training set for these models. In the next section this issue will be addressed by incresing the complexity of the Dense layers of the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving Letter and Fashion classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us modify slightly the model specific to each one of the latter datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specific part of the model for characters 2.0\n",
    "\n",
    "dropout1_c2=Dropout(0.4)(inputs2)\n",
    "dense1_c2=Dense(512,activation='relu')(dropout1_c2)\n",
    "dropout2_c2=Dropout(0.4)(dense1_c2)\n",
    "dense2_c2=Dense(256,activation='relu')(dropout2_c2)\n",
    "dropout3_c2=Dropout(0.4)(dense2_c2)\n",
    "dense3_c2=Dense(64,activation='relu')(dropout3_c2)\n",
    "output2_c2=Dense(27,activation='softmax')(dense3_c2)\n",
    "\n",
    "# Specific part of the model for fashion MNIST 2.0\n",
    "\n",
    "dropout1_f2=Dropout(0.4)(inputs2)\n",
    "dense1_f2=Dense(512,activation='relu')(dropout1_f2)\n",
    "dropout2_f2=Dropout(0.4)(dense1_f2)\n",
    "dense2_f2=Dense(256,activation='relu')(dropout2_f2)\n",
    "dropout3_f2=Dropout(0.4)(dense2_f2)\n",
    "dense3_f2=Dense(64,activation='relu')(dropout3_f2)\n",
    "output2_f2=Dense(10,activation='softmax')(dense3_f2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Letter Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Let us test the improved Letter recognition model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1.trainable = False\n",
    "conv2.trainable = False\n",
    "output1.trainable = False\n",
    "\n",
    "char_model_2=Model(inputs,output2_c2)\n",
    "plot_model(char_model_2,show_shapes=True,show_layer_names=True,expand_nested=True)\n",
    "char_model_2.compile(optimizer='rmsprop', loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "char_history_2 = char_model_2.fit(X_char,to_categorical(y_char),nb_epoch = 25,validation_split = 0.2,batch_size = 128,verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_curve(char_history_2,['Model Accuracy','Model Loss'],legend=['Train','Test'],metrics=['accuracy','loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_test_probs = char_model_2.predict(test_X_char)\n",
    "char_test_preds = [np.argmax(np.asarray(i)) for i in char_test_probs]\n",
    "filler=[0.0,0.0,0.0,0.0,0.0,0.0,0.0] #adds 0s to the labels from 21 to 27 because there are no samples with tese labels in the test set\n",
    "y_vals=np.asarray([list(i)+filler for i in to_categorical(test_y_char)])\n",
    "score_char, acc_char = char_model_2.evaluate(test_X_char,y_vals)\n",
    "print('The loss of the test data is {:.3f} with an accuracy of {:.3f}.'.format(score_char, acc_char))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_test_preds_df=pd.DataFrame()\n",
    "char_test_preds_df['Prediction']=char_test_preds\n",
    "char_test_preds_df['Label']=test_char_df['Label']\n",
    "char_missclassified=char_test_preds_df[char_test_preds_df.Prediction!=char_test_preds_df.Label]\n",
    "char_missclassified_index=char_missclassified.index.to_list()\n",
    "print('{} images (out of {}) were missclassified!'.format(len(char_missclassified_index),test_char_df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fashion Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1.trainable = False\n",
    "conv2.trainable = False\n",
    "output1.trainable = False\n",
    "\n",
    "fashion_model_2=Model(inputs,output2_f2)\n",
    "plot_model(fashion_model_2,show_shapes=True,show_layer_names=True,expand_nested=True)\n",
    "fashion_model_2.compile(optimizer='rmsprop', loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "fashion_history_2 = fashion_model_2.fit(X_fashion,to_categorical(y_fashion),nb_epoch = 25,validation_split = 0.2,batch_size = 32,verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_curve(fashion_history_2,['Model Accuracy','Model Loss'],legend=['Train','Test'],metrics=['accuracy','loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
